{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code inspired by https://www.kaggle.com/code/helmehelmuto/secondary-structure-prediction-with-keras\n",
    "#Dataset from https://www.kaggle.com/aashita/secondary-structure-of-protein-data-set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils import  to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393732, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ander\\AppData\\Local\\Temp\\ipykernel_16980\\2538759119.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array([[seq[i:i+n] for i in range(len(seq))] for seq in seqs])\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "data = pd.read_csv('./Data/2018-06-06-ss.cleaned.csv')\n",
    "max_length = 256\n",
    "print(data.shape)\n",
    "\n",
    "#Remove sequences that are too long and sequences that contain non-standard amino acids\n",
    "sequences, structures = data[['seq', 'sst3']][(data.len <= max_length)].values.T \n",
    "\n",
    "#For saving memory\n",
    "data = []\n",
    "\n",
    "#Making the sequences into grams, because this makes it easier for the model to learn\n",
    "def seq_2n_grams(seqs, n=3):\n",
    "    return np.array([[seq[i:i+n] for i in range(len(seq))] for seq in seqs])\n",
    "\n",
    "sequences = seq_2n_grams(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5021 8256   11    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Preprocess the data, to fit into the model\n",
    "\n",
    "#Encode the sequences with padding\n",
    "#This is important because the sequences are not all the same length, which\n",
    "#they need to be for the model to work\n",
    "tokenizer_seq = Tokenizer()\n",
    "tokenizer_seq.fit_on_texts(sequences)\n",
    "sequences = tokenizer_seq.texts_to_sequences(sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "print(sequences[1])\n",
    "\n",
    "#Encode the structures, and categorize them\n",
    "tokenizer_struc = Tokenizer(char_level=True)\n",
    "tokenizer_struc.fit_on_texts(structures)\n",
    "structures = tokenizer_struc.texts_to_sequences(structures)\n",
    "structures = pad_sequences(structures, maxlen=max_length, padding='post')\n",
    "structures = to_categorical(structures)\n",
    "\n",
    "print(structures[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 256, 128)          1198336   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 256, 128)         98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 256, 128)         98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256, 4)            516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,396,484\n",
      "Trainable params: 1,396,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1274/1274 [==============================] - 2092s 2s/step - loss: 0.3374 - accuracy: 0.8566 - val_loss: 0.2618 - val_accuracy: 0.8937\n",
      "Epoch 2/10\n",
      "1274/1274 [==============================] - 2043s 2s/step - loss: 0.2482 - accuracy: 0.8988 - val_loss: 0.2271 - val_accuracy: 0.9081\n",
      "Epoch 3/10\n",
      "1274/1274 [==============================] - 2052s 2s/step - loss: 0.2209 - accuracy: 0.9101 - val_loss: 0.2076 - val_accuracy: 0.9166\n",
      "Epoch 4/10\n",
      "1274/1274 [==============================] - 2054s 2s/step - loss: 0.2040 - accuracy: 0.9173 - val_loss: 0.1955 - val_accuracy: 0.9219\n",
      "Epoch 5/10\n",
      "1274/1274 [==============================] - 2053s 2s/step - loss: 0.1923 - accuracy: 0.9223 - val_loss: 0.1833 - val_accuracy: 0.9271\n",
      "Epoch 6/10\n",
      "1274/1274 [==============================] - 2058s 2s/step - loss: 0.1831 - accuracy: 0.9263 - val_loss: 0.1779 - val_accuracy: 0.9296\n",
      "Epoch 7/10\n",
      "1274/1274 [==============================] - 2055s 2s/step - loss: 0.1761 - accuracy: 0.9292 - val_loss: 0.1695 - val_accuracy: 0.9332\n",
      "Epoch 8/10\n",
      "1274/1274 [==============================] - 2061s 2s/step - loss: 0.1705 - accuracy: 0.9316 - val_loss: 0.1655 - val_accuracy: 0.9350\n",
      "Epoch 9/10\n",
      "1274/1274 [==============================] - 2064s 2s/step - loss: 0.1657 - accuracy: 0.9336 - val_loss: 0.1605 - val_accuracy: 0.9372\n",
      "Epoch 10/10\n",
      "1274/1274 [==============================] - 2058s 2s/step - loss: 0.1619 - accuracy: 0.9352 - val_loss: 0.1584 - val_accuracy: 0.9381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a9b6cbea90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the amount of words and tags, so we can use it in the model, with the correct dimensions\n",
    "input_dimention = len(tokenizer_seq.word_index) + 1\n",
    "tags = len(tokenizer_struc.word_index) + 1\n",
    "\n",
    "# Define the model architecture\n",
    "# Bidirectional LSTM is used because it is good for classifying sequences\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = input_dimention, output_dim= 128, input_length=max_length))\n",
    "model.add(Bidirectional(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Dense(tags, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, structures, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and save the model\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Uncomment this line to save the model\n",
    "#model.save(\"protein_model_smalldatasize.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.1584315150976181\n",
      "accuracy:  0.9381407499313354\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "eval = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"test loss:\", eval[0])\n",
    "print(\"accuracy: \", eval[1])\n",
    "\n",
    "model.save(\"protein_model_big_datasize.h5\")\n",
    "#Best model so far (protein_model2.h5) had 0.17 loss and 0.93 accuracy\n",
    "#That is the model used in the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e5e3ea4631e9d9d7acdd296c3802e1fa3f8d5be3021336b35a8944b5c5cae81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
